{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57c553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_Weights\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cv2\n",
    "import numpy as np\n",
    "from utils.general_utils import get_mask_from_rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56fbf361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes=2):\n",
    "    \"\"\"\n",
    "    Create a Mask R-CNN model pre-trained on COCO for instance segmentation, \n",
    "    customized for the desired number of classes.\n",
    "\n",
    "    :param num_classes: Number of output classes (including background). \n",
    "                        Default is 2 (background + ship).\n",
    "    :return: Mask R-CNN model with adjusted box and mask predictors.\n",
    "    \"\"\"\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n",
    "        weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "    )\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = \\\n",
    "        torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden = 256\n",
    "    model.roi_heads.mask_predictor = \\\n",
    "        torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(\n",
    "            in_features_mask, hidden, num_classes\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8205b264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)\n",
    "\n",
    "model = get_model()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/maskrcnn_train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796b11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.general_utils import get_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7d791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ShipDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for instance segmentation of ships.\n",
    "    Loads images lazily (one at a time) and decodes RLE masks on-demand.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rle_dict: dict[str, list[str]], transforms=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        :param rle_dict: Dictionary mapping image filenames to lists of RLE strings.\n",
    "                         Example: {\"img1.jpg\": [\"rle1\", \"rle2\", ...], ...}\n",
    "        :param transforms: Optional torchvision transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        self.rle_dict = rle_dict\n",
    "        self.transforms = transforms\n",
    "        self.img_ids = list(rle_dict.keys())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of images in the dataset.\n",
    "\n",
    "        :return: Number of images\n",
    "        \"\"\"\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Load one image and all its instance masks, returning the format expected by Mask R-CNN.\n",
    "\n",
    "        :param idx: Index of the image to load\n",
    "        :return: Tuple (image_tensor, target_dict) where\n",
    "                - image_tensor: torch.Tensor of shape [C, H, W], dtype=torch.float32\n",
    "                - target_dict: dict containing\n",
    "                - \"boxes\": Tensor[N, 4] of bounding boxes [x1, y1, x2, y2]\n",
    "                - \"labels\": Tensor[N] of class labels (1=ship)\n",
    "                - \"masks\": Tensor[N, H, W] of binary masks\n",
    "                - \"image_id\": Tensor[1] with image index\n",
    "                - \"area\": Tensor[N] of bounding box areas\n",
    "                - \"iscrowd\": Tensor[N] indicating crowd instances (all 0 here)\n",
    "        \"\"\"\n",
    "        img_id = self.img_ids[idx]\n",
    "        rle_list = self.rle_dict[img_id]\n",
    "\n",
    "        img = get_image(img_id)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img).permute(2, 0, 1)  # HWC â†’ CHW\n",
    "\n",
    "        masks = []\n",
    "        boxes = []\n",
    "\n",
    "        for rle in rle_list:\n",
    "            mask = get_mask_from_rle(rle).astype(np.uint8)\n",
    "\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            ys, xs = np.where(mask == 1)\n",
    "            if len(xs) == 0:\n",
    "                continue\n",
    "            \n",
    "            masks.append(mask)\n",
    "\n",
    "            x1, y1 = xs.min(), ys.min()\n",
    "            x2, y2 = xs.max(), ys.max()\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "         \n",
    "        if len(boxes) == 0:\n",
    "            return None\n",
    "\n",
    "        masks = torch.as_tensor(np.stack(masks), dtype=torch.uint8)  # [N,H,W]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)          # [N,4]\n",
    "        labels = torch.ones((len(boxes),), dtype=torch.int64)        # all 1 = ship class\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]),\n",
    "            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        return img_tensor, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f81571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fbe657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    data_loader: DataLoader,\n",
    "    device: str,\n",
    "    epoch: int\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train a Mask R-CNN model for one epoch.\n",
    "\n",
    "    :param model: Mask R-CNN model\n",
    "    :param optimizer: Optimizer (SGD, Adam, etc.)\n",
    "    :param data_loader: PyTorch DataLoader yielding (images, targets)\n",
    "    :param device: Device string, 'cuda' or 'cpu'\n",
    "    :param epoch: Current epoch number\n",
    "    :return: Average loss over the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss: float = 0.0\n",
    "\n",
    "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch}\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        \n",
    "        if batch is None:\n",
    "            continue\n",
    "\n",
    "        images, targets = batch\n",
    "        \n",
    "        images: list[torch.Tensor] = [img.to(device) for img in images]\n",
    "\n",
    "        targets: list[dict[str, torch.Tensor]] = [\n",
    "            {key: val.to(device) for key, val in t.items()}\n",
    "            for t in targets\n",
    "        ]\n",
    "\n",
    "        loss_dict: dict[str, torch.Tensor] = model(images, targets)\n",
    "\n",
    "        losses: torch.Tensor = sum(loss_dict.values(), torch.tensor(0.0, device=device))\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "\n",
    "        optimizer.zero_grad()   \n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "  \n",
    "        pbar.set_postfix(loss=float(losses.item()))\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c0cc76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs):\n",
    "    \"\"\"\n",
    "    Full training loop for Mask R-CNN.\n",
    "\n",
    "    :param model: Mask R-CNN model\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param val_loader: Optional DataLoader for validation data\n",
    "    :param epochs: Number of epochs to train\n",
    "    \"\"\"\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} - Loss: {train_loss:.4f}\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"checkpoints/maskrcnn_epoch_{epoch}.pth\")\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bba32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fd6f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.general_utils import load_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3f7f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_masks()\n",
    "\n",
    "rle_dict = (\n",
    "    df.groupby(\"ImageId\")[\"EncodedPixels\"]\n",
    "      .apply(list)\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "img_ids = list(rle_dict.keys())\n",
    "\n",
    "test_img_ids, temp_img_ids = train_test_split(img_ids, test_size=0.4, random_state=42)\n",
    "val_img_ids, train_img_ids = train_test_split(temp_img_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "train_rle_dict = {img_id: rle_dict[img_id] for img_id in train_img_ids}\n",
    "\n",
    "shipDataset = ShipDataset(rle_dict=train_rle_dict, transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64571de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c408a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch : list[tuple[NDArray, dict[str, NDArray]]]):\n",
    "    \"\"\"\n",
    "    Custom collate function for PyTorch DataLoader that filters out `None` samples\n",
    "    and combines a batch of (image, target) tuples into tuples of images and targets.\n",
    "\n",
    "    :param batch: List of (image, target) tuples, where\n",
    "                  - image: NumPy array of shape [H, W, C]\n",
    "                  - target: dictionary with keys like \"boxes\", \"labels\", \"masks\"\n",
    "    :return: Tuple of:\n",
    "             - images: tuple of NDArray images\n",
    "             - targets: tuple of dictionaries\n",
    "             Returns None if all batch elements are None.\n",
    "    \"\"\"\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5501db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 16/9628 [00:56<9:29:01,  3.55s/it, loss=0.735] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshipDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs)\u001b[39m\n\u001b[32m      2\u001b[39m os.makedirs(\u001b[33m\"\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     lr_scheduler.step()\n\u001b[32m      8\u001b[39m     writer.add_scalar(\u001b[33m\"\u001b[39m\u001b[33mLoss/train\u001b[39m\u001b[33m\"\u001b[39m, train_loss, epoch)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, optimizer, data_loader, device, epoch)\u001b[39m\n\u001b[32m     42\u001b[39m total_loss += losses.item()\n\u001b[32m     44\u001b[39m optimizer.zero_grad()   \n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mlosses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m optimizer.step()\n\u001b[32m     49\u001b[39m pbar.set_postfix(loss=\u001b[38;5;28mfloat\u001b[39m(losses.item()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Subjects/Deep-Learning/homework/venv/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Subjects/Deep-Learning/homework/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Subjects/Deep-Learning/homework/venv/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    train_loader=DataLoader(shipDataset, batch_size=4, shuffle=True, collate_fn=collate_fn),\n",
    "    val_loader=None,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f815a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from utils.general_utils import compute_iou_matrix, compute_average_f_score\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_model(model, rle_dict: Dict[str, List[str]], device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluate Mask R-CNN segmentation performance using average F2 score.\n",
    "\n",
    "    :param model: Mask R-CNN model\n",
    "    :param rle_dict: Dictionary mapping image IDs to lists of ground truth RLE strings\n",
    "                     Example: {\"img1.jpg\": [\"rle1\", \"rle2\", ...], ...}\n",
    "    :param device: Device string ('cuda' or 'cpu')\n",
    "    :return: Tuple containing:\n",
    "             - mean F2 score across all images\n",
    "             - list of F2 scores per image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    f2_scores = []\n",
    "\n",
    "    for img_id, gt_rles in tqdm(rle_dict.items(), desc=\"Evaluating\"):\n",
    "        img = get_image(img_id)\n",
    "        img_tensor = torch.from_numpy(img.astype(np.float32) / 255.).permute(2,0,1).to(device)\n",
    "\n",
    "        pred = model([img_tensor])[0]  \n",
    "        pred_masks = pred[\"masks\"].squeeze(1).cpu().numpy() > 0.5  # [N,H,W]\n",
    "\n",
    "        gt_masks = [get_mask_from_rle(rle).astype(np.uint8) for rle in gt_rles]\n",
    "\n",
    "        iou_mat = compute_iou_matrix(gt_masks, pred_masks)\n",
    "\n",
    "        f2 = compute_average_f_score(iou_mat)\n",
    "        f2_scores.append(f2)\n",
    "\n",
    "    return np.mean(f2_scores), f2_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f47a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "_mean_f2, f2_scores = evaluate_model(\n",
    "    model=model,\n",
    "    rle_dict={im_id: rle_dict[im_id] for im_id in val_img_ids},\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Mean F2 Score on Validation Set: {_mean_f2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
