{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "720a7b68",
   "metadata": {},
   "source": [
    "## UNet-Based Ship Segmentation Pipeline\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lorinctoldi/deep-learning/blob/main/model.ipynb)\n",
    "\n",
    "This notebook covers an end-to-end pipeline for ship segmentation using a UNet model. \n",
    "It includes dataset preparation, model creation, training, evaluation, and visualization of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96980dd5",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f4ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import load_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40282384",
   "metadata": {},
   "source": [
    "We load the masks into `masks_df` and replace empty strings with `NaN` values. \n",
    "This standardizes missing masks for images with no ships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_df = load_masks()\n",
    "masks_df['EncodedPixels'] = masks_df['EncodedPixels'].replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0664bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bfcb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019498ac",
   "metadata": {},
   "source": [
    "As the dataset is heavily imbalanced towards images with no ships (i.e., no masks), training directly on the full dataset would bias the model towards predicting empty images. To mitigate this, we first group images by the presence of masks. Images with at least one ship mask are treated as positive examples, while images with no masks are treated as negative examples.\n",
    "\n",
    "Next, we limit the sample size to 10,000 images, taking 5,000 images with ships and 5,000 empty images. This ensures a balanced subset that is representative but more manageable for training.\n",
    "\n",
    "Finally, we create a binary label for each image. Images containing at least one ship mask are assigned a label of 1, while empty images are assigned a label of 0. This simplifies balancing the dataset and splitting it for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f0889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels = masks_df.groupby('ImageId')['EncodedPixels'] \\\n",
    "            .apply(lambda x: 1 if x.notna().any() else 0) \\\n",
    "            .reset_index()\n",
    "\n",
    "image_labels = image_labels.rename(columns={'EncodedPixels': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_images = image_labels[image_labels['label'] == 1]\n",
    "empty_images = image_labels[image_labels['label'] == 0]\n",
    "\n",
    "SAMPLE_SIZE = 5000\n",
    "\n",
    "ship_sample = ship_images.sample(\n",
    "    n=min(SAMPLE_SIZE, len(ship_images)), random_state=42\n",
    ")\n",
    "empty_sample = empty_images.sample(\n",
    "    n=min(SAMPLE_SIZE, len(empty_images)), random_state=42\n",
    ")\n",
    "\n",
    "balanced_df = pd.concat([ship_sample, empty_sample], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03766e42",
   "metadata": {},
   "source": [
    "We split the balanced dataset into:\n",
    "- `train_df` (80%)  \n",
    "- `temp_df` (20%), which is further split into:  \n",
    "  - `val_df` (10%) for validation  \n",
    "  - `test_df` (10%) for final evaluation  \n",
    "\n",
    "Stratification ensures the proportion of ship vs empty images is consistent across splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd49ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(\n",
    "    balanced_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=balanced_df['label']\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=temp_df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c135340",
   "metadata": {},
   "source": [
    "To retrieve all masks for a given efficiently, we created a dictionary mapping each `ImageId` to a list of RLE-encoded masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dict = masks_df.groupby(\"ImageId\")[\"EncodedPixels\"].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from utils import get_mask_from_rle\n",
    "from constants import IMAGE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a35cd",
   "metadata": {},
   "source": [
    "Since the dataset has been split into training, validation, and test sets, we need a pipeline to efficiently load each image along with its corresponding mask. For each ImageId in the DataFrame, we read the image from disk and retrieve all associated RLE masks, combining them into a single binary mask. The images are then resized to 256x256 and normalized to values between 0 and 1, while the masks are resized to the same resolution and converted into binary tensors. Finally, this process is wrapped into a `tf.data.Dataset` pipeline. The training dataset is shuffled and batched to improve training efficiency, whereas the validation and test datasets are batched but not shuffled to ensure consistent evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_and_mask(img_id: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Load an image and its combined mask from disk.\n",
    "\n",
    "    :param img_id: Tensor containing the image filename\n",
    "    :return: Tuple of (image, mask) tensors with shapes (256,256,3) and (256,256,1)\n",
    "    \"\"\"\n",
    "    img_id = img_id.numpy().decode(\"utf-8\")\n",
    "\n",
    "    path = os.path.join(IMAGE_PATH, img_id)\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [768, 768])\n",
    "    img = img / 255.0\n",
    "\n",
    "    rles = mask_dict.get(img_id, [])\n",
    "    mask = np.zeros((768, 768), dtype=np.uint8)\n",
    "\n",
    "    for rle in rles:\n",
    "        if isinstance(rle, str):\n",
    "            mask += get_mask_from_rle(rle)\n",
    "\n",
    "    mask = tf.convert_to_tensor(mask[..., None], dtype=tf.float32)\n",
    "    mask = tf.image.resize(mask, [768, 768])\n",
    "    mask = tf.cast(mask > 0, tf.float32)\n",
    "\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d802bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_load_image_and_mask(img_id: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Load an image and its mask as TensorFlow tensors for use in a dataset.\n",
    "\n",
    "    :param img_id: Image ID tensor (string)\n",
    "    :return: Tuple of (image, mask) tensors with shapes (256,256,3) and (256,256,1)\n",
    "    \"\"\"\n",
    "    img, mask = tf.py_function(\n",
    "        load_image_and_mask,\n",
    "        [img_id],\n",
    "        [tf.float32, tf.float32]\n",
    "    )\n",
    "    img.set_shape([768, 768, 3])\n",
    "    mask.set_shape([768, 768, 1])\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df: pd.DataFrame, batch_size: int = 8, shuffle: bool = True) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Create a TensorFlow dataset of images and masks from a DataFrame.\n",
    "\n",
    "    :param df: DataFrame containing 'ImageId' column\n",
    "    :param batch_size: Number of samples per batch\n",
    "    :param shuffle: Whether to shuffle the dataset\n",
    "    :return: tf.data.Dataset yielding (image, mask) tuples\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(df[\"ImageId\"].values)\n",
    "    dataset = dataset.map(tf_load_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02029799",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_df)\n",
    "val_ds   = make_dataset(val_df, shuffle=False)\n",
    "test_ds  = make_dataset(test_df, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bae2f5",
   "metadata": {},
   "source": [
    "## Model Creation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e25c7a",
   "metadata": {},
   "source": [
    "## Model Creation and Training\n",
    "\n",
    "After trials and errors we decided to use the UNet architecture. It is a widely used CNN for image segmentation tasks, because it can capture both the overall structure and fine details of objects. \n",
    "\n",
    "We resized all images and masks to 256x256 to reduce memory usage and speed up training, even though using the full 768x768 resolution would likely produce more accurate segmentations. Similarly, we limited the dataset to a balanced sample of 10,000 images to save computational resources while maintaining a representative set of images with and without ships. Hyperparameters were chosen after testing multiple configurations to optimize segmentation performance.\n",
    "\n",
    "The model outputs a single-channel probability map for each image, indicating the likelihood that each pixel belongs to a ship. To train the model, we use a combined loss function that equally weights Binary Crossentropy and Dice loss. Binary Crossentropy encourages correct pixel-wise predictions, while Dice loss directly measures the overlap between the predicted and true masks, which is particularly useful for handling the class imbalance between ship and background pixels. We also monitor Intersection over Union (IoU) and F2 score as metrics, which provide a more holistic evaluation of segmentation quality by considering both precision and recall, rather than just pixel-wise accuracy.\n",
    "\n",
    "The Adam optimizer is used to update model parameters efficiently during training. Overall, this setup provides a balance between computational feasibility and segmentation performance, allowing us to experiment and evaluate the model within reasonable resource constraints.\n",
    "\n",
    "### Metrics: IoU and F2 Score\n",
    "\n",
    "Most of the images in this dataset are empty or mostly background. If we used simple accuracy as a metric, the model could just predict all zeros and still get a very high score. That wouldnâ€™t tell us anything about how well it actually detects ships. For this reason, we use Intersection over Union (IoU) and F2 score, which focus on how good the predicted masks are.\n",
    "\n",
    "**Intersection over Union (IoU):**\n",
    "IoU measures how much the predicted mask overlaps with the ground truth:\n",
    "\n",
    "$IoU = \\frac{\\text{Intersection}}{\\text{Union}} = \\frac{TP}{TP + FP + FN}$\n",
    "\n",
    "It basically counts how many pixels the prediction got right compared to the total pixels that should have been or were predicted as ships. A perfect mask gets 1, and no overlap gets 0. This helps us focus on actual ship pixels rather than the huge number of background pixels.\n",
    "\n",
    "**F2 Score:**\n",
    "The F-beta score combines precision and recall. We use F2, which weights recall more heavily because missing ship pixels is worse than predicting a few extra:\n",
    "\n",
    "$F_2 = (1 + 2^2) \\cdot \\frac{Precision \\cdot Recall}{(2^2 \\cdot Precision) + Recall}$\n",
    "\n",
    "- Precision: How many predicted ship pixels are actually ships.\n",
    "- Recall: How many true ship pixels we actually detected.\n",
    "\n",
    "F2 is high when the model finds most of the ship pixels, even if it makes a few extra predictions.\n",
    "\n",
    "Why not accuracy?\n",
    "Accuracy would mostly reflect background pixels and would make the model look better than it actually is. IoU and F2 give a more realistic picture of how well the model is actually detecting ships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true: tf.Tensor, y_pred: tf.Tensor, smooth: float = 1e-6) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the Dice loss for binary segmentation.\n",
    "\n",
    "    :param y_true: Ground truth mask tensor\n",
    "    :param y_pred: Predicted mask tensor\n",
    "    :param smooth: Small value to avoid division by zero\n",
    "    :return: Dice loss value\n",
    "    \"\"\"\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return 1 - (2 * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd04f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "def combined_loss(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Combine Binary Crossentropy and Dice loss for binary segmentation.\n",
    "\n",
    "    :param y_true: Ground truth mask tensor\n",
    "    :param y_pred: Predicted mask tensor\n",
    "    :return: Combined loss value\n",
    "    \"\"\"\n",
    "    return 0.5 * bce(y_true, y_pred) + 0.5 * dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_score_metric(y_true: tf.Tensor, y_pred: tf.Tensor, beta: float = 2, smooth: float = 1e-6) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the F-beta score for binary segmentation.\n",
    "\n",
    "    :param y_true: Ground truth mask tensor\n",
    "    :param y_pred: Predicted mask tensor\n",
    "    :param beta: Weight of recall relative to precision (default=2)\n",
    "    :param smooth: Small value to avoid division by zero\n",
    "    :return: F-beta score tensor\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    tp = tf.reduce_sum(y_true * y_pred)\n",
    "    fp = tf.reduce_sum(y_pred) - tp\n",
    "    fn = tf.reduce_sum(y_true) - tp\n",
    "\n",
    "    fbeta = (1 + beta**2) * tp / ((1 + beta**2)*tp + beta**2 * fn + fp + smooth)\n",
    "    return fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c829d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric(y_true: tf.Tensor, y_pred: tf.Tensor, smooth: float = 1e-6) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) for binary segmentation.\n",
    "\n",
    "    :param y_true: Ground truth mask tensor\n",
    "    :param y_pred: Predicted mask tensor\n",
    "    :param smooth: Small value to avoid division by zero\n",
    "    :return: IoU score tensor\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    return (intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_shape: tuple[int, int, int] = (768, 768, 3)) -> Model:\n",
    "    \"\"\"\n",
    "    Build a simple UNet model for binary image segmentation.\n",
    "\n",
    "    :param input_shape: Shape of the input image (height, width, channels)\n",
    "    :return: Compiled Keras Model with UNet architecture\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    c1 = layers.Conv2D(16, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(16, 3, activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPool2D()(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(32, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(32, 3, activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPool2D()(c2)\n",
    "\n",
    "    b = layers.Conv2D(64, 3, activation='relu', padding='same')(p2)\n",
    "    b = layers.Conv2D(64, 3, activation='relu', padding='same')(b)\n",
    "\n",
    "    u2 = layers.UpSampling2D()(b)\n",
    "    u2 = layers.Concatenate()([u2, c2])\n",
    "    c3 = layers.Conv2D(32, 3, activation='relu', padding='same')(u2)\n",
    "    c3 = layers.Conv2D(32, 3, activation='relu', padding='same')(c3)\n",
    "\n",
    "    u1 = layers.UpSampling2D()(c3)\n",
    "    u1 = layers.Concatenate()([u1, c1])\n",
    "    c4 = layers.Conv2D(16, 3, activation='relu', padding='same')(u1)\n",
    "    c4 = layers.Conv2D(16, 3, activation='relu', padding='same')(c4)\n",
    "\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(c4)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "model = unet()\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=combined_loss, metrics=[iou_metric, f2_score_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920dcd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"checkpoints/best_unet_model.h5\"\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    CHECKPOINT_PATH,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac0ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_iou = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_iou_metric\",\n",
    "    patience=3,\n",
    "    mode=\"max\",\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "early_stop_f2 = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_f2_score_metric\",\n",
    "    patience=3,\n",
    "    mode=\"max\",\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Checkpoint found at {CHECKPOINT_PATH}, skipping training.\")\n",
    "    model = load_model(\n",
    "        CHECKPOINT_PATH,\n",
    "        custom_objects={\n",
    "            \"iou_metric\": iou_metric,\n",
    "            \"f2_score_metric\": f2_score_metric,\n",
    "            \"combined_loss\": combined_loss\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    print(\"Checkpoint not found, training the model.\")\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=20,\n",
    "        callbacks=[checkpoint, early_stop_iou, early_stop_f2],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5986309e",
   "metadata": {},
   "source": [
    "## Test / Evaluation / Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7916fa",
   "metadata": {},
   "source": [
    "Once the UNet model is trained, we evaluate it on the held-out test set to see how well it generalizes. Since most images are mostly background, simple accuracy would be misleading, so we focus on metrics like IoU and F2 score, which better reflect how well the model detects ships and captures their shapes. The combined loss gives an overall sense of both pixel-level accuracy and mask overlap quality.\n",
    "\n",
    "For inference, we take each image, resize it to 256x256 for the model, and predict a pixel-wise probability map. This map is then resized back to the original 768x768 resolution and thresholded to produce a binary mask highlighting the ships.\n",
    "\n",
    "To understand how well the model performs, we visualize both curated examples and random test images. Curated picks illustrate strengths such as detecting low-contrast ships, handling multiple vessels, and correctly ignoring empty images, while also showing where minor noise or segmentation errors occur. Random samples give a broader view of performance across the dataset, helping us see both successes and typical failure modes in realistic scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d04f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_ds, return_dict=True)\n",
    "print(\"Test Loss:\", results[\"loss\"])\n",
    "print(\"Test IoU:\", results[\"iou_metric\"])\n",
    "print(\"Test F2 Score:\", results[\"f2_score_metric\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ca526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(model, img_id, threshold: float = 0.5):\n",
    "    img = get_image(img_id)\n",
    "\n",
    "    img_resized = tf.image.resize(img, [768, 768])\n",
    "    img_batch = tf.expand_dims(img_resized, 0)\n",
    "    pred_mask = model.predict(img_batch)[0]\n",
    "\n",
    "    pred_mask_full = tf.image.resize(pred_mask, [768, 768])\n",
    "    pred_mask_2d = tf.squeeze(pred_mask_full, axis=-1)\n",
    "\n",
    "    # Keep only pixels with probability >= threshold\n",
    "    pred_mask_2d = tf.cast(pred_mask_2d >= threshold, tf.float32)\n",
    "    \n",
    "    return pred_mask_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f7ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_image_with_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba4f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_picks = {\n",
    "    \"0a12e3118\": \"No ship detected, background correctly ignored.\",\n",
    "    \"0a8d5d261\": \"Single ship with high contrast detected accurately.\",\n",
    "    \"0a9fb0743\": \"Single ship partially out of frame still detected well.\",\n",
    "    \"0a3b48a9c\": \"Multiple ships detected, minor noise in mask.\",\n",
    "    \"0a9bc3e3a\": \"Connected ships detected as one, overall mask is good.\",\n",
    "    \"0a814feb5\": \"Ship with low contrast detected, some mask noise.\",\n",
    "    \"0a1174f25\": \"Two ships detected, major mask noise for partially covered ship.\",\n",
    "    \"0a286fb15\": \"Segmentation errors present, but ships partially detected.\",\n",
    "}\n",
    "\n",
    "for key in curated_picks.keys():\n",
    "    print(f\"{key}.jpg: {curated_picks[key]}\")\n",
    "    visualize_image_with_mask(key, predict_mask(model, key).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f86b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = test_df.sample(n=5)\n",
    "\n",
    "for sample in random_sample.ImageId:\n",
    "    img_id = sample.split('.')[0]\n",
    "    visualize_image_with_mask(img_id, predict_mask(model, img_id).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
