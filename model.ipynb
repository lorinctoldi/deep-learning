{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "720a7b68",
   "metadata": {},
   "source": [
    "## UNet-Based Ship Segmentation Pipeline\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lorinctoldi/deep-learning/blob/main/model.ipynb)\n",
    "\n",
    "This notebook covers an end-to-end pipeline for ship segmentation using a UNet model. \n",
    "It includes dataset preparation, model creation, training, evaluation, and visualization of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96980dd5",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2f4ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import load_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40282384",
   "metadata": {},
   "source": [
    "We load the masks into `masks_df` and replace empty strings with `NaN` values. \n",
    "This standardizes missing masks for images with no ships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "871f29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_df = load_masks()\n",
    "masks_df['EncodedPixels'] = masks_df['EncodedPixels'].replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0664bba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00003e153.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001124c7.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000155de5.jpg</td>\n",
       "      <td>264661 17 265429 33 266197 33 266965 33 267733...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId                                      EncodedPixels\n",
       "0  00003e153.jpg                                                NaN\n",
       "1  0001124c7.jpg                                                NaN\n",
       "2  000155de5.jpg  264661 17 265429 33 266197 33 266965 33 267733..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47bfcb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019498ac",
   "metadata": {},
   "source": [
    "As the dataset is heavily imbalanced towards images with no ships (i.e., no masks), training directly on the full dataset would bias the model towards predicting empty images. To mitigate this, we first group images by the presence of masks. Images with at least one ship mask are treated as positive examples, while images with no masks are treated as negative examples.\n",
    "\n",
    "Next, we limit the sample size to 10,000 images, taking 5,000 images with ships and 5,000 empty images. This ensures a balanced subset that is representative but more manageable for training.\n",
    "\n",
    "Finally, we create a binary label for each image. Images containing at least one ship mask are assigned a label of 1, while empty images are assigned a label of 0. This simplifies balancing the dataset and splitting it for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01f0889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels = masks_df.groupby('ImageId')['EncodedPixels'] \\\n",
    "            .apply(lambda x: 1 if x.notna().any() else 0) \\\n",
    "            .reset_index()\n",
    "\n",
    "image_labels = image_labels.rename(columns={'EncodedPixels': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e8c6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_images = image_labels[image_labels['label'] == 1]\n",
    "empty_images = image_labels[image_labels['label'] == 0]\n",
    "\n",
    "SAMPLE_SIZE = 5000\n",
    "\n",
    "ship_sample = ship_images.sample(\n",
    "    n=min(SAMPLE_SIZE, len(ship_images)), random_state=42\n",
    ")\n",
    "empty_sample = empty_images.sample(\n",
    "    n=min(SAMPLE_SIZE, len(empty_images)), random_state=42\n",
    ")\n",
    "\n",
    "balanced_df = pd.concat([ship_sample, empty_sample], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03766e42",
   "metadata": {},
   "source": [
    "We split the balanced dataset into:\n",
    "- `train_df` (80%)  \n",
    "- `temp_df` (20%), which is further split into:  \n",
    "  - `val_df` (10%) for validation  \n",
    "  - `test_df` (10%) for final evaluation  \n",
    "\n",
    "Stratification ensures the proportion of ship vs empty images is consistent across splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fd49ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(\n",
    "    balanced_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=balanced_df['label']\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=temp_df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c135340",
   "metadata": {},
   "source": [
    "To retrieve all masks for a given efficiently, we created a dictionary mapping each `ImageId` to a list of RLE-encoded masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4af4af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dict = masks_df.groupby(\"ImageId\")[\"EncodedPixels\"].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd8c695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from utils import get_mask_from_rle\n",
    "from constants import IMAGE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a35cd",
   "metadata": {},
   "source": [
    "Since the dataset has been split into training, validation, and test sets, we need a pipeline to efficiently load each image along with its corresponding mask. For each ImageId in the DataFrame, we read the image from disk and retrieve all associated RLE masks, combining them into a single binary mask. The images are then resized to 256x256 and normalized to values between 0 and 1, while the masks are resized to the same resolution and converted into binary tensors. Finally, this process is wrapped into a `tf.data.Dataset` pipeline. The training dataset is shuffled and batched to improve training efficiency, whereas the validation and test datasets are batched but not shuffled to ensure consistent evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf5d2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_and_mask(img_id: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Load an image and its combined mask from disk.\n",
    "\n",
    "    :param img_id: Tensor containing the image filename\n",
    "    :return: Tuple of (image, mask) tensors with shapes (256,256,3) and (256,256,1)\n",
    "    \"\"\"\n",
    "    img_id = img_id.numpy().decode(\"utf-8\")\n",
    "\n",
    "    path = os.path.join(IMAGE_PATH, img_id)\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [768, 768])\n",
    "    img = img / 255.0\n",
    "\n",
    "    rles = mask_dict.get(img_id, [])\n",
    "    mask = np.zeros((768, 768), dtype=np.uint8)\n",
    "\n",
    "    for rle in rles:\n",
    "        if isinstance(rle, str):\n",
    "            mask += get_mask_from_rle(rle)\n",
    "\n",
    "    mask = tf.convert_to_tensor(mask[..., None], dtype=tf.float32)\n",
    "    mask = tf.image.resize(mask, [768, 768])\n",
    "    mask = tf.cast(mask > 0, tf.float32)\n",
    "\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d802bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_load_image_and_mask(img_id: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Load an image and its mask as TensorFlow tensors for use in a dataset.\n",
    "\n",
    "    :param img_id: Image ID tensor (string)\n",
    "    :return: Tuple of (image, mask) tensors with shapes (256,256,3) and (256,256,1)\n",
    "    \"\"\"\n",
    "    img, mask = tf.py_function(\n",
    "        load_image_and_mask,\n",
    "        [img_id],\n",
    "        [tf.float32, tf.float32]\n",
    "    )\n",
    "    img.set_shape([768, 768, 3])\n",
    "    mask.set_shape([768, 768, 1])\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ca6c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df: pd.DataFrame, batch_size: int = 8, shuffle: bool = True) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Create a TensorFlow dataset of images and masks from a DataFrame.\n",
    "\n",
    "    :param df: DataFrame containing 'ImageId' column\n",
    "    :param batch_size: Number of samples per batch\n",
    "    :param shuffle: Whether to shuffle the dataset\n",
    "    :return: tf.data.Dataset yielding (image, mask) tuples\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(df[\"ImageId\"].values)\n",
    "    dataset = dataset.map(tf_load_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02029799",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_df)\n",
    "val_ds   = make_dataset(val_df, shuffle=False)\n",
    "test_ds  = make_dataset(test_df, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bae2f5",
   "metadata": {},
   "source": [
    "## Model Creation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e25c7a",
   "metadata": {},
   "source": [
    "## Model Creation and Training\n",
    "\n",
    "After trials and errors we decided to use the UNet architecture. It is a widely used CNN for image segmentation tasks, because it can capture both the overall structure and fine details of objects. \n",
    "\n",
    "We resized all images and masks to 256x256 to reduce memory usage and speed up training, even though using the full 768x768 resolution would likely produce more accurate segmentations. Similarly, we limited the dataset to a balanced sample of 10,000 images to save computational resources while maintaining a representative set of images with and without ships. Hyperparameters were chosen after testing multiple configurations to optimize segmentation performance.\n",
    "\n",
    "The model outputs a single-channel probability map for each image, indicating the likelihood that each pixel belongs to a ship. To train the model, we use a combined loss function that equally weights Binary Crossentropy and Dice loss. Binary Crossentropy encourages correct pixel-wise predictions, while Dice loss directly measures the overlap between the predicted and true masks, which is particularly useful for handling the class imbalance between ship and background pixels. We also monitor Intersection over Union (IoU) and F2 score as metrics, which provide a more holistic evaluation of segmentation quality by considering both precision and recall, rather than just pixel-wise accuracy.\n",
    "\n",
    "The Adam optimizer is used to update model parameters efficiently during training. Overall, this setup provides a balance between computational feasibility and segmentation performance, allowing us to experiment and evaluate the model within reasonable resource constraints.\n",
    "\n",
    "### Metrics: IoU and F2 Score\n",
    "\n",
    "Most of the images in this dataset are empty or mostly background. If we used simple accuracy as a metric, the model could just predict all zeros and still get a very high score. That wouldn’t tell us anything about how well it actually detects ships. For this reason, we use Intersection over Union (IoU) and F2 score, which focus on how good the predicted masks are.\n",
    "\n",
    "**Intersection over Union (IoU):**\n",
    "IoU measures how much the predicted mask overlaps with the ground truth:\n",
    "\n",
    "$IoU = \\frac{\\text{Intersection}}{\\text{Union}} = \\frac{TP}{TP + FP + FN}$\n",
    "\n",
    "It basically counts how many pixels the prediction got right compared to the total pixels that should have been or were predicted as ships. A perfect mask gets 1, and no overlap gets 0. This helps us focus on actual ship pixels rather than the huge number of background pixels.\n",
    "\n",
    "**F2 Score:**\n",
    "The F-beta score combines precision and recall. We use F2, which weights recall more heavily because missing ship pixels is worse than predicting a few extra:\n",
    "\n",
    "$F_2 = (1 + 2^2) \\cdot \\frac{Precision \\cdot Recall}{(2^2 \\cdot Precision) + Recall}$\n",
    "\n",
    "- Precision: How many predicted ship pixels are actually ships.\n",
    "- Recall: How many true ship pixels we actually detected.\n",
    "\n",
    "F2 is high when the model finds most of the ship pixels, even if it makes a few extra predictions.\n",
    "\n",
    "Why not accuracy?\n",
    "Accuracy would mostly reflect background pixels and would make the model look better than it actually is. IoU and F2 give a more realistic picture of how well the model is actually detecting ships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a09b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true: tf.Tensor, y_pred: tf.Tensor, smooth: float = 1e-6) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the Dice loss for binary segmentation.\n",
    "\n",
    "    :param y_true: Ground truth mask tensor\n",
    "    :param y_pred: Predicted mask tensor\n",
    "    :param smooth: Small value to avoid division by zero\n",
    "    :return: Dice loss value\n",
    "    \"\"\"\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return 1 - (2 * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd04f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "def combined_loss(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Combine Binary Crossentropy and Dice loss for binary segmentation.\n",
    "\n",
    "    :param y_true: Ground truth mask tensor\n",
    "    :param y_pred: Predicted mask tensor\n",
    "    :return: Combined loss value\n",
    "    \"\"\"\n",
    "    return 0.5 * bce(y_true, y_pred) + 0.5 * dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d03b0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_score_metric(y_true: tf.Tensor, y_pred: tf.Tensor, beta: float = 2, smooth: float = 1e-6) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the F-beta score for binary segmentation.\n",
    "\n",
    "    :param y_true: Ground truth mask tensor\n",
    "    :param y_pred: Predicted mask tensor\n",
    "    :param beta: Weight of recall relative to precision (default=2)\n",
    "    :param smooth: Small value to avoid division by zero\n",
    "    :return: F-beta score tensor\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    tp = tf.reduce_sum(y_true * y_pred)\n",
    "    fp = tf.reduce_sum(y_pred) - tp\n",
    "    fn = tf.reduce_sum(y_true) - tp\n",
    "\n",
    "    fbeta = (1 + beta**2) * tp / ((1 + beta**2)*tp + beta**2 * fn + fp + smooth)\n",
    "    return fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c829d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric(y_true: tf.Tensor, y_pred: tf.Tensor, smooth: float = 1e-6) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) for binary segmentation.\n",
    "\n",
    "    :param y_true: Ground truth mask tensor\n",
    "    :param y_pred: Predicted mask tensor\n",
    "    :param smooth: Small value to avoid division by zero\n",
    "    :return: IoU score tensor\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    return (intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "022c20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4dd9c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_shape: tuple[int, int, int] = (768, 768, 3)) -> Model:\n",
    "    \"\"\"\n",
    "    Build a simple UNet model for binary image segmentation.\n",
    "\n",
    "    :param input_shape: Shape of the input image (height, width, channels)\n",
    "    :return: Compiled Keras Model with UNet architecture\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    c1 = layers.Conv2D(16, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(16, 3, activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPool2D()(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(32, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(32, 3, activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPool2D()(c2)\n",
    "\n",
    "    b = layers.Conv2D(64, 3, activation='relu', padding='same')(p2)\n",
    "    b = layers.Conv2D(64, 3, activation='relu', padding='same')(b)\n",
    "\n",
    "    u2 = layers.UpSampling2D()(b)\n",
    "    u2 = layers.Concatenate()([u2, c2])\n",
    "    c3 = layers.Conv2D(32, 3, activation='relu', padding='same')(u2)\n",
    "    c3 = layers.Conv2D(32, 3, activation='relu', padding='same')(c3)\n",
    "\n",
    "    u1 = layers.UpSampling2D()(c3)\n",
    "    u1 = layers.Concatenate()([u1, c1])\n",
    "    c4 = layers.Conv2D(16, 3, activation='relu', padding='same')(u1)\n",
    "    c4 = layers.Conv2D(16, 3, activation='relu', padding='same')(c4)\n",
    "\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(c4)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "model = unet()\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=combined_loss, metrics=[iou_metric, f2_score_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "920dcd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47f8a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"checkpoints/768/best_unet_model.h5\" # retrain the model with 768x768 images\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    CHECKPOINT_PATH,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cb30a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4dac0ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_iou = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_iou_metric\",\n",
    "    patience=3,\n",
    "    mode=\"max\",\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "early_stop_f2 = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_f2_score_metric\",\n",
    "    patience=3,\n",
    "    mode=\"max\",\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "60ec48a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd6c67ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint not found, training the model.\n",
      "Epoch 1/20\n",
      "\u001b[1m   7/1000\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:04:37\u001b[0m 4s/step - f2_score_metric: 0.0149 - iou_metric: 0.0035 - loss: 0.8198"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint not found, training the model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop_iou\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop_f2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    398\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[1;32m--> 399\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    239\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    240\u001b[0m     ):\n\u001b[1;32m--> 241\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    243\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Checkpoint found at {CHECKPOINT_PATH}, skipping training.\")\n",
    "    model = load_model(\n",
    "        CHECKPOINT_PATH,\n",
    "        custom_objects={\n",
    "            \"iou_metric\": iou_metric,\n",
    "            \"f2_score_metric\": f2_score_metric,\n",
    "            \"combined_loss\": combined_loss\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    print(\"Checkpoint not found, training the model.\")\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=20,\n",
    "        callbacks=[checkpoint, early_stop_iou, early_stop_f2],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5986309e",
   "metadata": {},
   "source": [
    "## Test / Evaluation / Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7916fa",
   "metadata": {},
   "source": [
    "Once the UNet model is trained, we evaluate it on the held-out test set to see how well it generalizes. Since most images are mostly background, simple accuracy would be misleading, so we focus on metrics like IoU and F2 score, which better reflect how well the model detects ships and captures their shapes. The combined loss gives an overall sense of both pixel-level accuracy and mask overlap quality.\n",
    "\n",
    "For inference, we take each image, resize it to 256x256 for the model, and predict a pixel-wise probability map. This map is then resized back to the original 768x768 resolution and thresholded to produce a binary mask highlighting the ships.\n",
    "\n",
    "To understand how well the model performs, we visualize both curated examples and random test images. Curated picks illustrate strengths such as detecting low-contrast ships, handling multiple vessels, and correctly ignoring empty images, while also showing where minor noise or segmentation errors occur. Random samples give a broader view of performance across the dataset, helping us see both successes and typical failure modes in realistic scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d04f9d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_1\" is incompatible with the layer: expected shape=(None, 256, 256, 3), found shape=(None, 768, 768, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest IoU:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miou_metric\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Projects\\University\\deep-learning-labs\\satellite_imaging\\.venv\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"functional_1\" is incompatible with the layer: expected shape=(None, 256, 256, 3), found shape=(None, 768, 768, 3)"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_ds, return_dict=True)\n",
    "print(\"Test Loss:\", results[\"loss\"])\n",
    "print(\"Test IoU:\", results[\"iou_metric\"])\n",
    "print(\"Test F2 Score:\", results[\"f2_score_metric\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ca526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(model, img_id, threshold: float = 0.5):\n",
    "    img = get_image(img_id)\n",
    "\n",
    "    img_resized = tf.image.resize(img, [768, 768])\n",
    "    img_batch = tf.expand_dims(img_resized, 0)\n",
    "    pred_mask = model.predict(img_batch)[0]\n",
    "\n",
    "    pred_mask_full = tf.image.resize(pred_mask, [768, 768])\n",
    "    pred_mask_2d = tf.squeeze(pred_mask_full, axis=-1)\n",
    "\n",
    "    # Keep only pixels with probability >= threshold\n",
    "    pred_mask_2d = tf.cast(pred_mask_2d >= threshold, tf.float32)\n",
    "    \n",
    "    return pred_mask_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f7ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_image_with_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba4f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_picks = {\n",
    "    \"0a12e3118\": \"No ship detected, background correctly ignored.\",\n",
    "    \"0a8d5d261\": \"Single ship with high contrast detected accurately.\",\n",
    "    \"0a9fb0743\": \"Single ship partially out of frame still detected well.\",\n",
    "    \"0a3b48a9c\": \"Multiple ships detected, minor noise in mask.\",\n",
    "    \"0a9bc3e3a\": \"Connected ships detected as one, overall mask is good.\",\n",
    "    \"0a814feb5\": \"Ship with low contrast detected, some mask noise.\",\n",
    "    \"0a1174f25\": \"Two ships detected, major mask noise for partially covered ship.\",\n",
    "    \"0a286fb15\": \"Segmentation errors present, but ships partially detected.\",\n",
    "}\n",
    "\n",
    "for key in curated_picks.keys():\n",
    "    print(f\"{key}.jpg: {curated_picks[key]}\")\n",
    "    visualize_image_with_mask(key, predict_mask(model, key).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f86b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = test_df.sample(n=5)\n",
    "\n",
    "for sample in random_sample.ImageId:\n",
    "    img_id = sample.split('.')[0]\n",
    "    visualize_image_with_mask(img_id, predict_mask(model, img_id).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
