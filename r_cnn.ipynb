{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06cbb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_Weights\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cv2\n",
    "import numpy as np\n",
    "from util.general_utils import rle_to_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f257ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eaabd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes=2):\n",
    "    \"\"\"\n",
    "    Creates a Mask R-CNN model for instance segmentation.\n",
    "    num_classes = 2 → background + ship\n",
    "    \"\"\"\n",
    "    # Load the pretrained COCO model\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n",
    "        weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "    )\n",
    "\n",
    "    # Replace classification head (COCO has 91 classes)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features  # type: ignore\n",
    "    model.roi_heads.box_predictor = \\\n",
    "        torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Replace mask head (COCO has 91 classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels # type: ignore\n",
    "    hidden = 256\n",
    "    model.roi_heads.mask_predictor = \\\n",
    "        torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(\n",
    "            in_features_mask, hidden, num_classes\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e171124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)\n",
    "\n",
    "model = get_model()\n",
    "model.to(device)\n",
    "\n",
    "# Some people use Adam, but SGD works best for detection models\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# StepLR reduces the learning rate every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# TensorBoard logger\n",
    "writer = SummaryWriter(log_dir=\"runs/maskrcnn_train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80293a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ShipDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for instance segmentation of ships.\n",
    "    Loads images lazily (one at a time) and decodes RLE masks on-demand.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 rle_dict: dict[str, list[str]],\n",
    "                 image_root: str,\n",
    "                 transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rle_dict: {\"img1.jpg\": [\"rle1\", \"rle2\", ...], ...}\n",
    "            image_root: folder path: \"./data/images\"\n",
    "            transforms: torchvision transforms to apply to the image\n",
    "        \"\"\"\n",
    "        self.rle_dict = rle_dict\n",
    "        self.image_root = image_root\n",
    "        self.transforms = transforms\n",
    "        self.img_ids = list(rle_dict.keys())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Number of images in the dataset\"\"\"\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Load ONE image + all its instance masks.\n",
    "        Returns the exact format Mask R-CNN expects.\n",
    "        \"\"\"\n",
    "\n",
    "        img_id = self.img_ids[idx]\n",
    "        rles = self.rle_dict[img_id]\n",
    "\n",
    "        # -------------------\n",
    "        # 1. Load image lazily\n",
    "        # -------------------\n",
    "        img_path = f\"{self.image_root}/{img_id}\"\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(img_path)\n",
    "\n",
    "        img = img[:, :, ::-1]          # BGR → RGB\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img).permute(2, 0, 1)  # HWC → CHW\n",
    "\n",
    "        H, W = img_tensor.shape[1:]\n",
    "\n",
    "        # -------------------\n",
    "        # 2. Decode all RLE masks\n",
    "        # -------------------\n",
    "        masks = []\n",
    "        boxes = []\n",
    "\n",
    "        for rle in rles:\n",
    "            mask = rle_to_mask(rle, H, W).astype(np.uint8)\n",
    "            \n",
    "             # skip empty masks\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            # check if bounding box CAN be formed\n",
    "            ys, xs = np.where(mask == 1)\n",
    "            if len(xs) == 0:\n",
    "                continue  # skip mask entirely\n",
    "            \n",
    "            masks.append(mask)\n",
    "\n",
    "            x1, y1 = xs.min(), ys.min()\n",
    "            x2, y2 = xs.max(), ys.max()\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "        \n",
    "        # rcnn will go crazy if there are no valid objects   \n",
    "        if len(boxes) == 0:\n",
    "            # No valid objects in this image → skip and collate fn will handle it\n",
    "            return None\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        masks = torch.as_tensor(np.stack(masks), dtype=torch.uint8)  # [N,H,W]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)          # [N,4]\n",
    "        labels = torch.ones((len(boxes),), dtype=torch.int64)        # all 1 = ship class\n",
    "\n",
    "        # -------------------\n",
    "        # 3. Create COCO-style target dict\n",
    "        # -------------------\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]),\n",
    "            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        # -------------------\n",
    "        # 4. Apply transforms (we dont need them now)\n",
    "        # -------------------\n",
    "        # if self.transforms:\n",
    "        #     img_tensor = self.transforms(img_tensor)\n",
    "\n",
    "        return img_tensor, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3c96d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31098cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    data_loader: DataLoader,\n",
    "    device: str,\n",
    "    epoch: int\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Trains Mask R-CNN for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: Mask R-CNN model.\n",
    "        optimizer: SGD/Adam optimizer.\n",
    "        data_loader: PyTorch DataLoader that yields (images, targets).\n",
    "        device: 'cuda' or 'cpu'.\n",
    "        epoch: Current epoch number.\n",
    "\n",
    "    Returns:\n",
    "        Average loss over the epoch (float).\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    total_loss: float = 0.0\n",
    "\n",
    "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch}\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        \n",
    "        if batch is None:\n",
    "            continue  # skip this batch entirely\n",
    "\n",
    "        images, targets = batch\n",
    "        \n",
    "        images: list[torch.Tensor] = [img.to(device) for img in images]\n",
    "\n",
    "        targets: list[dict[str, torch.Tensor]] = [\n",
    "            {key: val.to(device) for key, val in t.items()}\n",
    "            for t in targets\n",
    "        ]\n",
    "\n",
    "        loss_dict: dict[str, torch.Tensor] = model(images, targets)\n",
    "\n",
    "        losses: torch.Tensor = sum(loss_dict.values(), torch.tensor(0.0, device=device))\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "\n",
    "        optimizer.zero_grad()   \n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "  \n",
    "        pbar.set_postfix(loss=float(losses.item()))\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "effe4913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7af0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs):\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "\n",
    "        # Step learning rate down\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} - Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(model.state_dict(), f\"checkpoints/maskrcnn_epoch_{epoch}.pth\")\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f4be858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9befa7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/segmentations.csv\")\n",
    "\n",
    "rle_dict = (\n",
    "    df.groupby(\"ImageId\")[\"EncodedPixels\"]\n",
    "      .apply(list)\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "im_ids = list(rle_dict.keys())\n",
    "\n",
    "im_ids[:3]\n",
    "\n",
    "test_im_ids, temp_im_ids = train_test_split(im_ids, test_size=0.4, random_state=42)\n",
    "val_im_ids, train_im_ids = train_test_split(temp_im_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "train_rle_dict = {im_id: rle_dict[im_id] for im_id in train_im_ids}\n",
    "\n",
    "\n",
    "shipDataset = ShipDataset(rle_dict=train_rle_dict,\n",
    "                          image_root=\"./data/images\",\n",
    "                          transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aacd1fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80ed6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch : list[tuple[NDArray, dict[str, NDArray]]]):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None  # tell the train loop to skip this batch\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0797dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/9628 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9628/9628 [51:30<00:00,  3.12it/s, loss=0.176]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.3682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9628/9628 [40:10<00:00,  3.99it/s, loss=0.699]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Loss: 0.3162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9628/9628 [33:10<00:00,  4.84it/s, loss=0.585] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Loss: 0.3002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9628/9628 [32:51<00:00,  4.88it/s, loss=0.442] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Loss: 0.2575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9628/9628 [33:20<00:00,  4.81it/s, loss=0.476] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Loss: 0.2470\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    train_loader=DataLoader(shipDataset, batch_size=4, shuffle=True, collate_fn=collate_fn),\n",
    "    val_loader=None,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d000e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from util.general_utils import compute_iou_matrix, average_f_score_of_image, rles_to_masks\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_model(model, rle_dict: Dict[str, List[str]], image_root: str, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluates Faster-RCNN segmentation performance using F2 score.\n",
    "    rle_dict maps ImageId -> list of GT RLE strings.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    f2_scores = []\n",
    "\n",
    "    for img_id, gt_rles in tqdm(rle_dict.items(), desc=\"Evaluating\"):\n",
    "        # --- Load image ---\n",
    "        img_path = f\"{image_root}/{img_id}\"\n",
    "        img_d = cv2.imread(img_path)\n",
    "        \n",
    "        if img_d is None:\n",
    "            raise FileNotFoundError(img_path)\n",
    "        \n",
    "        img = img_d[:, :, ::-1]  # BGR→RGB\n",
    "        img_tensor = torch.from_numpy(img.astype(np.float32) / 255.).permute(2,0,1).to(device)\n",
    "        \n",
    "        # --- Run model ---\n",
    "        pred = model([img_tensor])[0]  \n",
    "        pred_masks = pred[\"masks\"].squeeze(1).cpu().numpy() > 0.5  # [N,H,W]\n",
    "\n",
    "        # --- Prepare GT masks ---\n",
    "        H, W = img_tensor.shape[1:]\n",
    "        gt_masks = rles_to_masks(gt_rles, H, W)\n",
    "\n",
    "        # --- Compute IoU matrix ---\n",
    "        iou_mat = compute_iou_matrix(gt_masks, pred_masks)\n",
    "\n",
    "        # --- Compute F2 score for this image ---\n",
    "        f2 = average_f_score_of_image(iou_mat)\n",
    "        f2_scores.append(f2)\n",
    "\n",
    "    return np.mean(f2_scores), f2_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "592be329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 38511/38511 [1:02:18<00:00, 10.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F2 Score on Validation Set: 0.0811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_mean_f2, f2_scores = evaluate_model(\n",
    "    model=model,\n",
    "    rle_dict={im_id: rle_dict[im_id] for im_id in val_im_ids},\n",
    "    image_root=\"./data/images\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "print(f\"Mean F2 Score on Validation Set: {_mean_f2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
